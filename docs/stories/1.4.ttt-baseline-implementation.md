# Story 1.4: TTT Baseline Implementation

## Status

Draft

## Story

**As a** developer,
**I want** to implement Test-Time Training with a 1B parameter model,
**so that** I can establish a working baseline quickly.

## Acceptance Criteria

1. Successfully fork and adapt MIT TTT codebase
2. 1B model loads and runs on 16GB GPU
3. Basic LoRA adaptation working
4. Achieve 40%+ accuracy on validation set
5. Checkpoint saving/loading implemented
6. Training completes in under 2 hours
7. Memory usage stays under 10GB

## Tasks / Subtasks

- [ ] MIT TTT Codebase Integration (AC: 1)
  - [ ] Fork MIT TTT repository and create adaptation layer
  - [ ] Create `src/adapters/strategies/ttt_adapter.py` for integration
  - [ ] Adapt TTT codebase to work with ARC data models
  - [ ] Integrate with existing data pipeline from Story 1.2
  - [ ] Create configuration adapter for TTT parameters
- [ ] 1B Model Implementation (AC: 2, 7)
  - [ ] Create `src/domain/services/ttt_service.py` for TTT model management
  - [ ] Implement Llama-3.2 1B model loading with quantization
  - [ ] Add memory optimization and monitoring utilities
  - [ ] Implement GPU memory management with 16GB constraints
  - [ ] Add memory profiling and optimization features
- [ ] LoRA Adaptation System (AC: 3)
  - [ ] Create `src/utils/lora_adapter.py` for LoRA implementation
  - [ ] Implement basic LoRA layer insertion and management (rank=8, alpha=16 as starting values)
  - [ ] Add LoRA parameter optimization and tuning
  - [ ] Create LoRA checkpoint management system
  - [ ] Implement adapter weight merging utilities
- [ ] Training Pipeline (AC: 4, 6)
  - [ ] Create `src/domain/services/training_orchestrator.py`
  - [ ] Implement training loop with validation accuracy tracking
  - [ ] Add early stopping and convergence detection
  - [ ] Implement gradient accumulation and memory optimization
  - [ ] Create training progress monitoring and logging
  - [ ] Validate accuracy using ARC-AGI official validation set (100 tasks)
- [ ] Checkpoint Management (AC: 5)
  - [ ] Create `src/adapters/repositories/checkpoint_repository.py`
  - [ ] Implement checkpoint saving/loading with metadata
  - [ ] Add checkpoint versioning and management
  - [ ] Create checkpoint validation and integrity checks
  - [ ] Implement checkpoint cleanup and storage optimization
- [ ] Performance Validation (AC: 4, 6, 7)
  - [ ] Create performance benchmarking utilities
  - [ ] Implement accuracy validation against 40% target
  - [ ] Add training time monitoring (2-hour limit)
  - [ ] Create memory usage validation (10GB limit)
  - [ ] Implement comprehensive performance reporting
- [ ] Testing and Integration (All ACs)
  - [ ] Create unit tests for TTT adapter functionality
  - [ ] Create integration tests with data pipeline
  - [ ] Create performance tests for memory and time constraints
  - [ ] Create end-to-end TTT training validation tests

## Dev Notes

### Previous Story Insights

From Story 1.2 completion, the data pipeline provides:

- High-performance data loading (0.3-0.5s for 1000 tasks)
- Comprehensive caching system with LRU eviction
- Data augmentation capabilities with semantic preservation
- Memory-efficient sparse matrix operations suitable for TTT input processing
- Multiple batching strategies including adaptive batching for training

From Story 1.3 design (parallel development), the evaluation framework will provide:

- Pixel-perfect accuracy calculation for validation
- Real-time performance monitoring capabilities
- Experiment tracking and result analysis
- Resource usage monitoring for memory and time constraints

### Data Models and Structures

**TTT-Specific Models**: [Source: docs/architecture/data-models.md#core-domain-models]

```python
@dataclass
class TTTAdaptation:
    """Store Test-Time Training adaptations"""
    adaptation_id: str
    task_id: str
    base_model_checkpoint: str
    adapted_weights_path: str
    training_examples: List[Dict[str, Any]]
    adaptation_metrics: Dict[str, float]
    created_at: datetime

@dataclass
class ResourceUsage:
    task_id: str
    strategy_type: StrategyType
    cpu_seconds: float
    memory_mb: float
    gpu_memory_mb: Optional[float]
    api_calls: Dict[str, int]
    total_tokens: int
    estimated_cost: float
    timestamp: datetime
```

**Model Configuration**: [Source: configs/development.yaml]

```yaml
model:
  name: "llama-3.2-1b"  # Using Llama 3.2 1B for baseline
  device: "auto"
  quantization: true
  max_length: 2048
  batch_size: 16
  
training:
  epochs: 5
  learning_rate: 1e-4
  batch_size: 4
  gradient_accumulation_steps: 2
  warmup_steps: 100
  
resources:
  max_memory_gb: 10      # Adjusted to 10GB for TTT
  max_concurrent_tasks: 2
  max_processing_time: 7200  # Adjusted to 2 hours (7200s) for TTT
```

### Technology Stack

**ML/AI Technologies**: [Source: docs/architecture/tech-stack.md#aiml-stack]

- **Base Model**: Llama-3.2 1B (baseline implementation) with future scaling to Llama-3.1 8B
- **Inference**: Transformers, vLLM for model loading and inference
- **Training**: PyTorch 2.0+ for TTT implementation
- **Vector Store**: ChromaDB (embedded) for similarity search

**Data Processing**: [Source: docs/architecture/tech-stack.md#data-processing]

- **Serialization**: msgpack, orjson for efficient checkpoint serialization
- **Validation**: Pydantic v2 for model configuration validation
- **Caching**: diskcache for persistent checkpoint caching

### File Locations and Project Structure

**TTT Implementation Components**: [Source: docs/architecture/source-tree.md]

- `src/domain/services/ttt_service.py` - Core TTT model management and training orchestration
- `src/domain/services/training_orchestrator.py` - Training pipeline coordination and monitoring
- `src/adapters/strategies/ttt_adapter.py` - Integration adapter for MIT TTT codebase
- `src/adapters/repositories/checkpoint_repository.py` - Checkpoint storage and management
- `src/utils/lora_adapter.py` - LoRA implementation and management utilities

**Configuration and Storage**: [Source: docs/architecture/source-tree.md]

- `configs/strategies/ttt.yaml` - TTT-specific configuration parameters
- `data/models/` - Model checkpoints and TTT adaptations storage
- `data/cache/` - Cached TTT training artifacts and intermediate results
- Test files: `tests/unit/adapters/strategies/` for TTT adapter tests
- Test files: `tests/integration/` for TTT training pipeline tests

### Technical Requirements

**Performance Constraints**:

- 1B model must load and run on 16GB GPU hardware (AC: 2)
- Memory usage must stay under 10GB during training (AC: 7)
- Training must complete in under 2 hours for baseline validation (AC: 6)
- Must achieve 40%+ accuracy on validation set (AC: 4)

**Integration Requirements**:

- Must integrate with existing data pipeline from Story 1.2
- Must work with evaluation framework (Story 1.3) for accuracy validation
- Must support checkpoint saving/loading for experiment continuity (AC: 5)
- Must integrate with MIT TTT codebase while maintaining project architecture (AC: 1)

**Platform Compatibility**: [Source: configs/ multiple files]

- Must work across Kaggle (16GB GPU), Colab (15GB GPU), and local environments
- Must support quantization for memory efficiency
- Must handle varying GPU memory constraints gracefully

### MIT TTT Integration Strategy

**Adaptation Approach** (AC: 1):

- Fork MIT TTT repository and create integration layer in `src/adapters/strategies/`
- Create adapter pattern to translate between ARC data models and TTT expected formats
- Maintain original TTT algorithm integrity while integrating with project architecture
- Use configuration injection to adapt TTT parameters for ARC-specific requirements

**Code Integration Patterns**:

- Use dependency injection to integrate TTT components
- Create abstraction layer for model loading and checkpoint management
- Implement adapter pattern for data format translation
- Use factory pattern for TTT strategy instantiation

### LoRA Implementation Details

**LoRA Architecture** (AC: 3):

- Implement basic LoRA adapter insertion into Llama model layers
- Create configurable rank (default: 8) and alpha (default: 16) parameters for LoRA adaptation
- Implement LoRA weight initialization and optimization
- Support LoRA checkpoint saving and loading independently from base model

**Memory Optimization**:

- Use LoRA to reduce trainable parameters by 90%+ for memory efficiency
- Implement gradient checkpointing to reduce memory footprint
- Use mixed precision training (FP16) to optimize memory usage
- Implement dynamic batch sizing based on available memory

### Training Pipeline Architecture

**Training Orchestration** (AC: 4, 6):

- Implement training loop with real-time accuracy monitoring
- Create early stopping based on validation performance plateaus
- Use gradient accumulation to handle memory constraints
- Implement checkpoint saving at regular intervals and best performance points

**Performance Monitoring**:

- Track memory usage throughout training with alerts at thresholds
- Monitor training progress against 2-hour time limit
- Implement real-time accuracy tracking with 40% target validation
- Create performance profiling for bottleneck identification

### Coding Standards

**TTT-Specific Standards**: [Source: docs/architecture/coding-standards.md#python-style-guide]

- Use Black formatter with line length 100
- Use ruff for linting and import sorting
- Type hints required for all ML function signatures
- Document memory and performance characteristics in docstrings
- Use dataclasses for TTT configuration and result structures

**Competition-Focused Standards**: [Source: docs/architecture/coding-standards.md#competition-focused-standards]

- Mark experimental TTT optimizations with clear TODO markers
- Document performance trade-offs and memory considerations
- Use clear variable names for model parameters and hyperparameters
- Include timing and memory profiling in critical training functions

### Technical Constraints and Considerations

**Hardware Constraints**:

- 16GB GPU memory limit requires careful memory management
- 10GB system memory limit during training requires optimization
- 2-hour training time limit requires efficient implementation
- Platform-specific optimizations may be necessary for Kaggle/Colab

**Model Constraints**:

- 1B parameter model size for baseline (will scale to 8B in Story 1.5)
- Must maintain compatibility with Llama-3.2 architecture
- LoRA adaptation must be computationally efficient
- Quantization required for memory efficiency

**Performance Requirements**:

- 40% accuracy target on validation set is aggressive for baseline
- Training convergence must be achieved within time constraints
- Memory usage monitoring and optimization critical for platform compatibility
- Checkpoint management must be efficient to minimize I/O overhead

## Testing

**Test File Locations**: [Source: docs/architecture/source-tree.md]

- Unit tests: `tests/unit/adapters/strategies/` for TTT adapter tests
- Unit tests: `tests/unit/domain/services/` for TTT service tests
- Unit tests: `tests/unit/utils/` for LoRA adapter tests
- Integration tests: `tests/integration/test_ttt_pipeline.py`
- Integration tests: `tests/integration/test_ttt_data_integration.py`

**Testing Standards**: [Source: docs/architecture/test-strategy.md]

- Use pytest framework exclusively
- Follow testing pyramid distribution (70% unit, 25% integration, 5% E2E)
- All tests must pass before story completion
- Include performance benchmarks in test assertions

**Specific Testing Requirements for This Story**:

- Unit tests for TTT model loading and memory management
- Unit tests for LoRA adapter functionality and weight management
- Unit tests for checkpoint saving/loading and validation
- Integration tests for TTT training pipeline with real data
- Performance tests for memory usage (10GB limit) and training time (2-hour limit)
- Integration tests with data pipeline from Story 1.2

**Enhanced Testing Requirements**:

- Memory stress tests: validate 10GB limit under various training conditions
- Performance benchmarks: validate 2-hour training time with different batch sizes
- Accuracy validation tests: track progress toward 40% target using ARC-AGI official validation set
- Platform compatibility tests: validate across Kaggle, Colab, and local environments
- Checkpoint integrity tests: validate checkpoint loading after training interruption
- MIT TTT integration tests: validate adapter functionality with original codebase

## Change Log

| Date       | Version | Description            | Author       |
| ---------- | ------- | ---------------------- | ------------ |
| 2025-01-22 | 1.0     | Initial story creation | Scrum Master |

## Dev Agent Record

_This section will be populated by the development agent during implementation_

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results

_This section will be populated by the QA Agent during review_