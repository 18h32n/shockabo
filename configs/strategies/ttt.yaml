# MIT TTT (Test-Time Training) Strategy Configuration
# Based on MIT research: "The Surprising Effectiveness of Test-Time Training for Abstract Reasoning"
#
# PARAMETER GUIDE (Task 8.4):
# - MIT Research Parameters: Core methodology from MIT paper (learning rates, epochs, augmentation)
# - Competition Optimizations: ARC Prize 2025-specific enhancements for 58%+ accuracy
# - Inference Optimizations: Epic 3 enhancements for <5 minute inference time
# - Memory Optimizations: 24GB GPU constraints with selective checkpointing

model:
  name: "meta-llama/Llama-3-8B"  # 8B parameter model as per MIT research
  device: "auto"  # Automatically select GPU/CPU
  quantization: true  # Enable quantization for memory efficiency
  load_in_4bit: true  # Enable 4-bit quantization (QLoRA)
  bnb_4bit_quant_type: "nf4"  # NF4 quantization format
  bnb_4bit_compute_dtype: "float16"  # Compute dtype for quantized layers
  bnb_4bit_use_double_quant: true  # Double quantization for better memory
  max_length: 2048
  cache_dir: "data/models"
  mixed_precision: true  # Enable BF16 mixed precision as per MIT research
  use_flash_attention: true  # Enable Flash Attention for efficiency

training:
  # MIT TTT Training hyperparameters for 8B model - Enhanced for 58%+ validation accuracy (Epic 3)
  
  # Learning rates (Task 8.7 - MIT Research Parameters)
  learning_rate: 5e-5  # MIT base learning rate for fine-tuning
  per_instance_lr: 1e-4  # MIT learning rate for per-instance adaptation
  
  # Epochs (Epic 3 Optimization - Task 3.6)
  num_epochs: 2  # Optimized: 2-3 epochs (MIT: 3) for 20-30% faster training
  per_instance_epochs: 1  # Optimized: 1 epoch for per-instance (MIT: 2) 
  
  # Batch configuration (MIT Research + Memory Optimization)
  batch_size: 1  # Memory efficient for 8B model (24GB constraint)
  gradient_accumulation_steps: 4  # Effective batch size = 4 (MIT research)
  
  # Training optimization (Task 8.7 - Epic 3 LoRA Enhancements)
  warmup_ratio: 0.1  # 10% warmup with cosine decay (MIT research)
  max_grad_norm: 1.0  # Gradient clipping for stability (MIT research)
  
  # MIT TTT specific settings for 8B model - Epic 3: 58%+ validation target (Task 8.9)
  max_training_time: 300  # 5 minutes per task (Epic 3 optimization vs MIT: 30 min)
  target_accuracy: 0.58  # Epic 3 target: 58%+ accuracy (MIT baseline: 53%)
  memory_limit_mb: 24576  # 24GB memory limit for 8B model
  max_examples: 8  # Increased training examples for better learning
  validation_split: 0.2  # Use 20% of training data for validation
  early_stopping_patience: 3  # Epic 3: Reduced patience for faster convergence (MIT: 5)
  
  # Optimization settings (MIT research) - Enhanced for 8B
  mixed_precision: true  # Enable BF16 mixed precision
  gradient_checkpointing: true  # Enable gradient checkpointing for memory
  checkpointing_layers: 6  # NEW (Task 8.3): Checkpoint every 6 layers for selective memory optimization
  use_flash_attention: true  # Enable Flash Attention for efficiency
  dataloader_num_workers: 0  # Reduce workers for memory efficiency

lora:
  # QLoRA adaptation settings for 8B model - Optimized for 53%+ accuracy
  rank: 64  # Rank 64 for 8B model capacity (as per MIT research)
  alpha: 32  # Increased alpha for better adaptation strength
  dropout: 0.05  # Reduced dropout for better performance
  target_modules:  # Optimized for Llama-3 8B architecture
    # Attention projections (primary targets for QLoRA)
    - "q_proj"    # Query projection
    - "v_proj"    # Value projection
    - "k_proj"    # Key projection
    - "o_proj"    # Output projection
    # MLP projections (secondary targets)
    - "gate_proj" # Gate projection
    - "up_proj"   # Up projection
    - "down_proj" # Down projection
    # GPT-2 Conv1D modules (fallback compatibility)
    - "c_attn"   # Combined query, key, value projection
    - "c_proj"   # Output projection
  bias: "none"
  # QLoRA-specific settings - Enhanced for 8B model performance
  use_quantization: true
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"  # NF4 quantization for best quality
  bnb_4bit_compute_dtype: "bfloat16"  # BF16 for better numerical stability
  bnb_4bit_use_double_quant: true  # Double quantization for memory efficiency
  bnb_4bit_quant_storage: "uint8"  # Efficient storage

adaptation:
  # MIT TTT Task adaptation settings - Enhanced for 58%+ validation accuracy (Epic 3)
  
  # Leave-one-out configuration (Task 8.5 - MIT Research Parameter)
  use_leave_one_out: true  # MIT: Use N-1 examples for training, 1 for validation
  # Creates N adaptation runs for N training examples, improves generalization by 3-5%
  
  # Augmentation strategy (MIT Research Parameters)
  augmentation_strategy: "enhanced"  # Enhanced strategy for better accuracy
  use_basic_augmentation: true  # MIT basic augmentations (rotation, flip)
  use_size_augmentation: true  # Competition: size-based augmentations for diversity
  use_chain_augmentation: true  # Competition: chained augmentations for robustness
  augmentation_probability: 0.7  # 70% chance to apply augmentations
  
  # Self-consistency configuration (Task 8.6 - MIT Research + Epic 3 Enhancement)
  permute_n: 3  # Number of geometric permutations (rotations, flips) for voting
  # MIT: permute_n=1 (baseline), Epic 3: permute_n=3-5 (2-4% accuracy improvement)
  use_self_consistency: true  # Enable majority voting across permutations
  consensus_threshold: 0.6  # Require 60% agreement for high-confidence prediction
  # Consensus threshold determines confidence score: 0.6 = moderate, 0.8 = high confidence
  
  # Data processing - Optimized for 8B model
  use_gpt_format: true  # MIT uses GPT-style message formatting
  text_representation: "structured_grid"  # Enhanced grid representation
  include_reasoning: true  # Include reasoning steps for better performance
  
checkpoints:
  # Checkpoint management
  save_dir: "data/models/checkpoints"
  max_checkpoints_per_task: 5
  max_total_size_gb: 50
  save_best_only: true
  save_on_interrupt: true

inference:
  # MIT TTT Inference settings - Optimized for 58%+ accuracy with <5 minute inference
  temperature: 0.0  # Deterministic for exact matching
  top_p: 0.95
  top_k: 50  # Top-k sampling for better quality
  max_new_tokens: 300  # Reduced for efficient grid generation (Task 5.5)
  num_return_sequences: 1
  do_sample: false  # Deterministic decoding
  repetition_penalty: 1.1  # Prevent repetitive outputs
  no_repeat_ngram_size: 2  # Avoid n-gram repetition
  
  # MIT TTT specific inference - Enhanced
  use_adapted_model: true  # Use per-instance adapted model
  aggregate_predictions: true  # Aggregate multiple augmented predictions
  use_beam_search: false  # Greedy decoding for consistency
  early_stopping: true  # Stop when EOS is generated
  
  # NEW: Inference optimization parameters (Epic 3 enhancements - Task 8.2)
  max_inference_time: 300  # 300 seconds (5 minutes) strict timeout per task
  enable_torch_compile: true  # Enable JIT compilation for 15-20% speedup
  enable_kv_cache_optimization: true  # Enable KV-cache for 10-15% speedup
  enable_static_cache: true  # Pre-allocate cache for 5-10% speedup
  inference_batch_size: 1  # Batch size for inference (balance speed vs memory)
  
monitoring:
  # MIT TTT Performance monitoring
  log_frequency: 1  # Log every step for TTT
  profile_memory: true
  profile_time: true
  save_metrics: true
  metrics_file: "logs/mit_ttt_metrics.json"
  
  # MIT TTT specific monitoring
  track_adaptation_metrics: true
  track_memory_usage: true
  track_per_instance_performance: true
  save_adaptation_history: true

# Platform-specific overrides for 8B model QLoRA
platform_overrides:
  kaggle:
    model:
      name: "meta-llama/Llama-3-8B"  # Full 8B model on Kaggle
      load_in_4bit: true
      bnb_4bit_quant_type: "nf4"
      use_flash_attention: true
    training:
      batch_size: 1  # Memory constraint for Kaggle
      gradient_accumulation_steps: 2
      per_instance_epochs: 1
      memory_limit_mb: 15360  # 15GB for Kaggle GPU
    lora:
      rank: 64  # Full rank for better performance
    adaptation:
      use_chain_augmentation: false
  
  colab:
    model:
      name: "meta-llama/Llama-3-8B"  # 8B model on Colab if memory allows
      load_in_4bit: true
      bnb_4bit_quant_type: "nf4"
      gradient_checkpointing: true
    training:
      batch_size: 1
      gradient_accumulation_steps: 4  # Higher accumulation for memory
      mixed_precision: true
      gradient_checkpointing: true
      memory_limit_mb: 15360  # 15GB for Colab GPU
    lora:
      rank: 32  # Reduced rank for memory constraints
    adaptation:
      permute_n: 1  # Single permutation for speed
  
  local_gpu:
    model:
      name: "meta-llama/Llama-3-8B"  # Full 8B model locally
      load_in_4bit: true
      bnb_4bit_quant_type: "nf4"
      use_flash_attention: true
    training:
      batch_size: 2  # Larger batch if memory allows
      gradient_accumulation_steps: 1
      per_instance_epochs: 2
      memory_limit_mb: 24576  # 24GB for local GPU
    lora:
      rank: 64  # Full MIT configuration
    adaptation:
      permute_n: 3  # Multiple permutations for better results
      use_chain_augmentation: true
  
  # Fallback configuration for insufficient memory
  low_memory:
    model:
      name: "meta-llama/Llama-3.2-1B"  # Fallback to smaller model
      load_in_4bit: false  # No quantization needed for 1B
    training:
      batch_size: 2
      gradient_accumulation_steps: 1
      memory_limit_mb: 4096  # 4GB memory limit
    lora:
      rank: 16  # Very low rank for efficiency
    adaptation:
      permute_n: 1
      use_chain_augmentation: false

# USAGE EXAMPLES (Task 8.8 - Inline code examples showing enhanced TTT usage)
#
# Example 1: Basic enhanced TTT usage
# ```python
# from src.adapters.strategies.ttt_adapter import TTTAdapter
# from src.domain.models import ARCTask
#
# adapter = TTTAdapter()  # Loads config from this file
# task = ARCTask.from_dict(task_data, "task_001", "evaluation")
# result = await adapter.solve_task(task)
# print(f"Confidence: {result.confidence_score:.2%}")
# print(f"Prediction shape: {result.predicted_output.shape}")
# ```
#
# Example 2: Using enhanced TTT with ensemble interface
# ```python
# from src.adapters.strategies.ttt_adapter import TTTAdapter
# from src.domain.ports.strategy import StrategyPort
#
# # TTTAdapter implements StrategyPort for ensemble integration
# strategy: StrategyPort = TTTAdapter()
#
# # Quick confidence estimate for routing (<100ms)
# confidence = strategy.get_confidence_estimate(task)
# if confidence > 0.7:
#     result = await strategy.solve_task(task)
# ```
#
# Example 3: Performance monitoring and resource estimation
# ```python
# adapter = TTTAdapter()
#
# # Estimate resources before execution (<50ms)
# resources = adapter.get_resource_estimate(task)
# print(f"Estimated time: {resources.estimated_time_sec}s")
# print(f"Memory required: ~24GB (8B model)")
#
# # Execute with monitoring
# result = await adapter.solve_task(task)
# print(f"Actual time: {result.execution_time_ms}ms")
# print(f"Per-pixel confidence available: {result.per_pixel_confidence is not None}")
# ```
#
# EXPECTED PERFORMANCE CHARACTERISTICS (Task 8.9 - Epic 3 targets):
# - Standalone accuracy: 58%+ on evaluation set (100 tasks)
# - Inference time: <300 seconds (5 minutes) per task including adaptation
# - Memory usage: <24GB peak for 8B model with 4-bit quantization
# - Improvement over baseline: +3-5% from leave-one-out + self-consistency + LoRA optimization