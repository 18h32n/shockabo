{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced TTT Strategy - GPU Validation\n",
    "\n",
    "This notebook validates the Enhanced TTT Strategy implementation on Kaggle GPU.\n",
    "\n",
    "**Target Metrics:**\n",
    "- Accuracy: 58%+ on 100 evaluation tasks\n",
    "- Inference Time: <5 minutes per task\n",
    "- Performance Speedup: 30-40% from optimizations\n",
    "\n",
    "**Runtime: ~6-9 hours total**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KAGGLE ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "print(f\"GPU Count: {gpu_count}\")\n",
    "\n",
    "if cuda_available:\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: No GPU detected! Enable GPU in notebook settings.\")\n",
    "    print(\"Settings → Accelerator → GPU T4 x2\")\n",
    "\n",
    "# Check Kaggle environment\n",
    "print(f\"\\nKaggle Working Dir: {os.path.exists('/kaggle/working')}\")\n",
    "print(f\"Kaggle Input Dir: {os.path.exists('/kaggle/input')}\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced repository setup with multiple fallback options\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport time\n\nprint(\"=\" * 60)\nprint(\"REPOSITORY SETUP WITH FALLBACK OPTIONS\")\nprint(\"=\" * 60)\n\n# Repository configuration\nREPO_URL = \"https://github.com/18h32n/shockabo.git\"\nTARGET_DIR = \"/kaggle/working/arc-prize-2025\"\n\ndef run_command(cmd, description, timeout=300):\n    \"\"\"Run command with proper error handling and timeout.\"\"\"\n    print(f\"\\n{description}...\")\n    try:\n        result = subprocess.run(\n            cmd, shell=True, capture_output=True, text=True, \n            timeout=timeout, encoding='utf-8', errors='replace'\n        )\n        if result.returncode == 0:\n            print(f\"✓ {description} successful\")\n            if result.stdout.strip():\n                print(f\"Output: {result.stdout.strip()}\")\n            return True\n        else:\n            print(f\"✗ {description} failed (code: {result.returncode})\")\n            if result.stderr.strip():\n                print(f\"Error: {result.stderr.strip()}\")\n            return False\n    except subprocess.TimeoutExpired:\n        print(f\"✗ {description} timed out after {timeout}s\")\n        return False\n    except Exception as e:\n        print(f\"✗ {description} exception: {e}\")\n        return False\n\n# Option 1: Try fresh clone\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTION 1: Fresh Repository Clone\")\nprint(\"=\"*60)\n\n# Remove existing directory if it exists\nif os.path.exists(TARGET_DIR):\n    print(f\"Removing existing directory: {TARGET_DIR}\")\n    run_command(f\"rm -rf {TARGET_DIR}\", \"Directory cleanup\")\n\n# Try cloning\nclone_success = run_command(\n    f\"git clone {REPO_URL} {TARGET_DIR}\",\n    \"Git clone\",\n    timeout=600\n)\n\nif clone_success:\n    # Verify clone worked\n    if os.path.exists(f\"{TARGET_DIR}/src/utils/ttt_methodology.py\"):\n        print(\"✓ Repository clone verified - source files found\")\n    else:\n        print(\"✗ Repository clone incomplete - missing source files\")\n        clone_success = False\n\n# Option 2: Try alternative clone method\nif not clone_success:\n    print(\"\\n\" + \"=\"*60)\n    print(\"OPTION 2: Alternative Clone Method\")\n    print(\"=\"*60)\n    \n    run_command(f\"mkdir -p {TARGET_DIR}\", \"Create target directory\")\n    os.chdir(\"/kaggle/working\")\n    \n    alt_success = run_command(\n        f\"git clone --depth 1 --branch master {REPO_URL} arc-prize-2025\",\n        \"Shallow clone attempt\"\n    )\n    clone_success = alt_success\n\n# Option 3: Manual verification and setup\nif not clone_success:\n    print(\"\\n\" + \"=\"*60)\n    print(\"OPTION 3: Manual Fallback Setup\")\n    print(\"=\"*60)\n    print(\"❌ Repository clone failed. Creating minimal structure...\")\n    \n    # Create basic directory structure\n    os.makedirs(f\"{TARGET_DIR}/src/utils\", exist_ok=True)\n    os.makedirs(f\"{TARGET_DIR}/tests/integration\", exist_ok=True)\n    os.makedirs(f\"{TARGET_DIR}/scripts/platform_deploy\", exist_ok=True)\n    \n    print(\"⚠️  CRITICAL: Repository access failed!\")\n    print(\"   This will prevent the enhanced TTT validation from running properly.\")\n    print(\"   Please check repository access or upload code as Kaggle dataset.\")\n\n# Change to project directory regardless\ntry:\n    os.chdir(TARGET_DIR)\n    print(f\"\\n✓ Changed to directory: {os.getcwd()}\")\nexcept Exception as e:\n    print(f\"✗ Failed to change directory: {e}\")\n\n# Verify final state\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFICATION AND SETUP\")\nprint(\"=\"*60)\n\nprint(\"Current directory contents:\")\ntry:\n    for item in sorted(os.listdir(\".\")):\n        path = Path(item)\n        if path.is_dir():\n            print(f\"  📁 {item}/\")\n        else:\n            print(f\"  📄 {item}\")\nexcept Exception as e:\n    print(f\"Error listing directory: {e}\")\n\n# Run setup script if it exists\nsetup_script = Path(\"scripts/platform_deploy/kaggle_setup.py\")\nif setup_script.exists():\n    print(f\"\\n✓ Setup script found: {setup_script}\")\n    run_command(\n        \"python scripts/platform_deploy/kaggle_setup.py\",\n        \"Kaggle platform setup\",\n        timeout=600\n    )\nelse:\n    print(f\"\\n⚠️  Setup script not found: {setup_script}\")\n    print(\"Creating minimal environment setup...\")\n    \n    # Set critical environment variables manually\n    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n    os.environ['TORCH_USE_CUDA_DSA'] = '1' \n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n    print(\"✓ Set critical CUDA environment variables\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SETUP COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install test dependencies\n",
    "!pip install -q pytest pytest-asyncio pytest-timeout pytest-mock\n",
    "\n",
    "# Install transformers and related packages\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.25.0 bitsandbytes>=0.41.0 peft>=0.7.0\n",
    "\n",
    "# Verify installations\n",
    "import pytest\n",
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "\n",
    "print(f\"pytest: {pytest.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"accelerate: {accelerate.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(\"\\n✓ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify ARC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ARC dataset in Kaggle input\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Common locations for ARC dataset in Kaggle\n",
    "dataset_locations = [\n",
    "    \"/kaggle/input/arc-prize-2025\",\n",
    "    \"/kaggle/input/abstraction-and-reasoning-corpus\",\n",
    "    \"/kaggle/input/arc-agi\",\n",
    "    \"arc-prize-2025/data/downloaded\"  # Local copy\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for loc in dataset_locations:\n",
    "    test_path = Path(loc)\n",
    "    if test_path.exists():\n",
    "        print(f\"Found dataset at: {loc}\")\n",
    "        dataset_path = test_path\n",
    "        break\n",
    "\n",
    "if dataset_path:\n",
    "    # Look for evaluation files\n",
    "    eval_files = list(dataset_path.glob(\"**/arc-agi_evaluation_challenges.json\"))\n",
    "    solution_files = list(dataset_path.glob(\"**/arc-agi_evaluation_solutions.json\"))\n",
    "    \n",
    "    if eval_files and solution_files:\n",
    "        eval_path = eval_files[0]\n",
    "        solution_path = solution_files[0]\n",
    "        \n",
    "        # Load and verify\n",
    "        with open(eval_path) as f:\n",
    "            challenges = json.load(f)\n",
    "        with open(solution_path) as f:\n",
    "            solutions = json.load(f)\n",
    "        \n",
    "        print(f\"\\n✓ Dataset verified!\")\n",
    "        print(f\"  Challenges: {len(challenges)} tasks\")\n",
    "        print(f\"  Solutions: {len(solutions)} tasks\")\n",
    "        print(f\"  Evaluation path: {eval_path}\")\n",
    "        print(f\"  Solutions path: {solution_path}\")\n",
    "        \n",
    "        # Create symlink or copy to expected location\n",
    "        expected_dir = Path(\"arc-prize-2025/data/downloaded\")\n",
    "        expected_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        import shutil\n",
    "        shutil.copy(eval_path, expected_dir / \"arc-agi_evaluation_challenges.json\")\n",
    "        shutil.copy(solution_path, expected_dir / \"arc-agi_evaluation_solutions.json\")\n",
    "        print(f\"\\n✓ Dataset copied to: {expected_dir}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Dataset files not found in {dataset_path}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ ARC dataset not found!\")\n",
    "    print(\"Please add ARC dataset to your notebook:\")\n",
    "    print(\"  1. Click '+ Add Data' button\")\n",
    "    print(\"  2. Search for 'ARC Prize 2025' or 'abstraction reasoning'\")\n",
    "    print(\"  3. Click 'Add' and re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run Enhanced TTT Accuracy Validation\n",
    "\n",
    "**This will take 4-6 hours to complete**\n",
    "\n",
    "Tests 100 evaluation tasks with:\n",
    "- Leave-one-out generation\n",
    "- Self-consistency validation\n",
    "- LoRA optimization\n",
    "- Memory efficient batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced TTT accuracy test with comprehensive diagnostics\nimport subprocess\nimport sys\nimport os\nfrom pathlib import Path\nimport time\n\nprint(\"=\" * 60)\nprint(\"ENHANCED TTT ACCURACY VALIDATION\")\nprint(\"=\" * 60)\nprint(\"This will take 4-6 hours to complete (or fail quickly if issues persist)\")\n\n# Pre-flight checks\nprint(\"\\n\" + \"=\"*50)\nprint(\"PRE-FLIGHT DIAGNOSTICS\")\nprint(\"=\"*50)\n\n# Check current directory and files\ncurrent_dir = Path.cwd()\nprint(f\"Current directory: {current_dir}\")\n\n# Check for test file\ntest_file = Path(\"tests/integration/test_enhanced_ttt_accuracy.py\")\nprint(f\"Test file exists: {test_file.exists()}\")\n\nif test_file.exists():\n    print(f\"✓ Test file found: {test_file}\")\n    file_size = test_file.stat().st_size\n    print(f\"  File size: {file_size} bytes\")\nelse:\n    print(f\"✗ Test file NOT found: {test_file}\")\n    print(\"Available test files:\")\n    test_dir = Path(\"tests\")\n    if test_dir.exists():\n        for f in test_dir.rglob(\"*.py\"):\n            print(f\"  - {f}\")\n    else:\n        print(\"  No tests directory found!\")\n\n# Check for source files\ncritical_files = [\n    \"src/utils/ttt_methodology.py\",\n    \"src/utils/lora_adapter.py\", \n    \"src/adapters/strategies/ttt_adapter.py\"\n]\n\nprint(f\"\\nCritical source files:\")\nall_files_exist = True\nfor file_path in critical_files:\n    exists = Path(file_path).exists()\n    print(f\"  {'✓' if exists else '✗'} {file_path}\")\n    if not exists:\n        all_files_exist = False\n\n# Check environment variables\nprint(f\"\\nCUDA Environment Variables:\")\ncuda_vars = ['CUDA_LAUNCH_BLOCKING', 'TORCH_USE_CUDA_DSA', 'CUDA_VISIBLE_DEVICES']\nfor var in cuda_vars:\n    value = os.environ.get(var, 'Not set')\n    print(f\"  {var}: {value}\")\n\n# Check CUDA availability\ntry:\n    import torch\n    print(f\"\\nPyTorch CUDA Status:\")\n    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"  Device count: {torch.cuda.device_count()}\")\n        print(f\"  Current device: {torch.cuda.current_device()}\")\n        print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\nexcept Exception as e:\n    print(f\"  Error checking CUDA: {e}\")\n\n# Determine execution strategy\nprint(\"\\n\" + \"=\"*50)\nprint(\"EXECUTION STRATEGY\")\nprint(\"=\"*50)\n\nif not all_files_exist:\n    print(\"❌ CRITICAL: Missing source files - cannot run full validation\")\n    print(\"This suggests repository setup failed.\")\n    print(\"The test will likely fail immediately.\")\n    print(\"\\nTo fix this:\")\n    print(\"1. Check repository access\")\n    print(\"2. Verify GitHub URL is correct\") \n    print(\"3. Consider uploading code as Kaggle dataset\")\n    \nelif not test_file.exists():\n    print(\"❌ CRITICAL: Test file missing - cannot run validation\")\n    print(\"Even if source files exist, the test runner won't work.\")\n    \nelse:\n    print(\"✓ All pre-flight checks passed\")\n    print(\"Proceeding with enhanced TTT validation...\")\n\n# Execute the test with enhanced error handling\nprint(\"\\n\" + \"=\"*50)\nprint(\"TEST EXECUTION\")\nprint(\"=\"*50)\n\nstart_time = time.time()\n\ntry:\n    # Run the test with comprehensive logging\n    cmd = [\n        sys.executable, \"-m\", \"pytest\", \n        \"tests/integration/test_enhanced_ttt_accuracy.py::test_enhanced_ttt_accuracy\",\n        \"-v\", \"-s\", \"--no-header\", \"--tb=long\",  # Changed to long traceback\n        \"--capture=no\",  # Don't capture output\n        \"--timeout=25200\"  # 7 hour timeout\n    ]\n    \n    print(f\"Executing command: {' '.join(cmd)}\")\n    print(f\"Working directory: {os.getcwd()}\")\n    print(f\"Python executable: {sys.executable}\")\n    \n    # Run with real-time output\n    process = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1,\n        universal_newlines=True\n    )\n    \n    # Print output in real-time\n    while True:\n        output = process.stdout.readline()\n        if output == '' and process.poll() is not None:\n            break\n        if output:\n            print(output.strip())\n    \n    return_code = process.poll()\n    execution_time = time.time() - start_time\n    \n    print(f\"\\n\" + \"=\"*50)\n    print(\"TEST EXECUTION SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Execution time: {execution_time:.1f} seconds ({execution_time/60:.1f} minutes)\")\n    print(f\"Return code: {return_code}\")\n    \n    if return_code == 0:\n        print(\"✓ Enhanced TTT Accuracy Test PASSED\")\n    else:\n        print(f\"⚠️ Enhanced TTT Accuracy Test returned code: {return_code}\")\n        \n        # Provide specific guidance based on execution time\n        if execution_time < 300:  # Less than 5 minutes\n            print(\"⚠️ QUICK FAILURE DETECTED\")\n            print(\"This suggests a fundamental setup or CUDA issue.\")\n            print(\"Check the logs above for CUDA errors, setup failures, or missing files.\")\n        elif execution_time < 3600:  # Less than 1 hour  \n            print(\"⚠️ EARLY TERMINATION\")\n            print(\"Test started but terminated early. Check for memory issues or errors.\")\n        else:\n            print(\"ℹ️ Long execution detected - check logs for completion status.\")\n        \nexcept subprocess.TimeoutExpired:\n    execution_time = time.time() - start_time\n    print(f\"\\n⚠️ Test timed out after {execution_time:.1f} seconds\")\n    print(\"This could indicate the test is running normally (if close to 7 hours)\")\n    print(\"Or stuck in an infinite loop (if much shorter)\")\n    \nexcept Exception as e:\n    execution_time = time.time() - start_time\n    print(f\"\\n✗ Test execution failed after {execution_time:.1f} seconds: {e}\")\n    print(\"This indicates a fundamental execution environment issue.\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ENHANCED TTT ACCURACY TEST COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Analyze Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nprint(\"=\" * 60)\nprint(\"ENHANCED TTT ACCURACY RESULTS\")\nprint(\"=\" * 60)\n\nreport_path = Path(\"validation_results/enhanced_ttt_report.json\")\n\nif not report_path.exists():\n    print(\"⚠️ Results file not found!\")\n    print(f\"Expected: {report_path}\")\n    \n    # Try to find alternative result files\n    results_dir = Path(\"validation_results\")\n    if results_dir.exists():\n        print(\"\\nAvailable result files:\")\n        for f in results_dir.glob(\"*.json\"):\n            print(f\"  - {f}\")\n    else:\n        print(\"No results directory found\")\n        \n    # Check if test ran at all\n    if Path(\"test_output.log\").exists():\n        print(\"\\nTest appears to have run, check logs for issues\")\n    else:\n        print(\"\\nTest may not have completed successfully\")\n        \nelse:\n    try:\n        with open(report_path) as f:\n            report = json.load(f)\n        \n        summary = report.get('summary', {})\n        \n        print(f\"\\nTotal Tasks: {summary.get('total_tasks', 'N/A')}\")\n        print(f\"Correct Tasks: {summary.get('correct_tasks', 'N/A')}\")\n        \n        accuracy = summary.get('accuracy', 0)\n        print(f\"\\nAccuracy: {accuracy:.2%}\")\n        print(f\"Target (58%): {'✓ PASS' if accuracy >= 0.58 else '✗ FAIL'}\")\n        \n        print(f\"\\nAverage Confidence: {summary.get('avg_confidence', 0):.3f}\")\n        print(f\"Average Adaptation Time: {summary.get('avg_adaptation_time_sec', 0):.1f}s\")\n        print(f\"Average Inference Time: {summary.get('avg_inference_time_sec', 0):.1f}s\")\n        \n        total_avg_time = summary.get('avg_adaptation_time_sec', 0) + summary.get('avg_inference_time_sec', 0)\n        print(f\"\\nTotal Average Time: {total_avg_time:.1f}s ({total_avg_time/60:.1f} min)\")\n        print(f\"Time Target (<5 min): {'✓ PASS' if total_avg_time < 300 else '✗ FAIL'}\")\n        \n        print(f\"\\nTotal Runtime: {summary.get('total_time_sec', 0)/3600:.2f} hours\")\n        \n        # Difficulty breakdown\n        if 'difficulty_breakdown' in report:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"DIFFICULTY BREAKDOWN\")\n            print(\"=\" * 60)\n            for difficulty, metrics in report['difficulty_breakdown'].items():\n                print(f\"\\n{difficulty.upper()}:\")\n                print(f\"  Tasks: {metrics.get('total', 'N/A')}\")\n                print(f\"  Correct: {metrics.get('correct', 'N/A')}\")\n                print(f\"  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n                print(f\"  Avg Confidence: {metrics.get('avg_confidence', 0):.3f}\")\n                print(f\"  Avg Time: {metrics.get('avg_time', 0):.1f}s\")\n        \n        print(\"\\n\" + \"=\" * 60)\n        \n    except Exception as e:\n        print(f\"✗ Error reading results file: {e}\")\n        print(\"File may be corrupted or incomplete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run Performance Optimization Benchmarks\n",
    "\n",
    "**This will take 2-3 hours to complete**\n",
    "\n",
    "Benchmarks optimization techniques:\n",
    "- KV-cache optimization\n",
    "- Static cache\n",
    "- torch.compile\n",
    "- Combined speedup measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance optimization test\n",
    "!pytest tests/performance/test_ttt_inference_time.py::test_combined_optimizations \\\n",
    "    -v -s --no-header --tb=short \\\n",
    "    --timeout=14400  # 4 hour timeout\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE OPTIMIZATION TEST COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Analyze Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "perf_report_path = Path(\"validation_results/ttt_inference_optimization_report.json\")\n",
    "\n",
    "if perf_report_path.exists():\n",
    "    with open(perf_report_path) as f:\n",
    "        perf_report = json.load(f)\n",
    "    \n",
    "    speedup = perf_report['speedup_summary']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PERFORMANCE OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nBaseline Average Time: {speedup['avg_baseline_sec']:.1f}s\")\n",
    "    print(f\"Optimized Average Time: {speedup['avg_optimized_sec']:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nSpeedup Factor: {speedup['speedup_factor']:.2f}x\")\n",
    "    print(f\"Time Reduction: {speedup['reduction_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTarget Speedup (1.30x): {'✓ PASS' if speedup['speedup_factor'] >= 1.30 else '✗ FAIL'}\")\n",
    "    print(f\"Target Time (<5 min): {'✓ PASS' if speedup['avg_optimized_sec'] < 300 else '✗ FAIL'}\")\n",
    "    \n",
    "    print(f\"\\nBaseline Samples: {speedup['baseline_samples']}\")\n",
    "    print(f\"Optimized Samples: {speedup['optimized_samples']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Performance results file not found!\")\n",
    "    print(f\"Expected: {perf_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Final Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Platform: Kaggle\")\n",
    "\n",
    "# Check accuracy results\n",
    "accuracy_report = Path(\"validation_results/enhanced_ttt_report.json\")\n",
    "perf_report = Path(\"validation_results/ttt_inference_optimization_report.json\")\n",
    "\n",
    "all_passed = True\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ACCURACY VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if accuracy_report.exists():\n",
    "    with open(accuracy_report) as f:\n",
    "        acc_data = json.load(f)\n",
    "    \n",
    "    accuracy = acc_data['summary']['accuracy']\n",
    "    avg_time = (acc_data['summary']['avg_adaptation_time_sec'] + \n",
    "                acc_data['summary']['avg_inference_time_sec'])\n",
    "    \n",
    "    acc_pass = accuracy >= 0.58\n",
    "    time_pass = avg_time < 300\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2%} - {'✓ PASS' if acc_pass else '✗ FAIL'} (target: 58%+)\")\n",
    "    print(f\"Avg Time: {avg_time:.1f}s - {'✓ PASS' if time_pass else '✗ FAIL'} (target: <300s)\")\n",
    "    \n",
    "    if not (acc_pass and time_pass):\n",
    "        all_passed = False\n",
    "else:\n",
    "    print(\"✗ FAIL - No accuracy report found\")\n",
    "    all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"PERFORMANCE OPTIMIZATION VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if perf_report.exists():\n",
    "    with open(perf_report) as f:\n",
    "        perf_data = json.load(f)\n",
    "    \n",
    "    speedup_factor = perf_data['speedup_summary']['speedup_factor']\n",
    "    opt_time = perf_data['speedup_summary']['avg_optimized_sec']\n",
    "    \n",
    "    speedup_pass = speedup_factor >= 1.30\n",
    "    opt_time_pass = opt_time < 300\n",
    "    \n",
    "    print(f\"Speedup: {speedup_factor:.2f}x - {'✓ PASS' if speedup_pass else '✗ FAIL'} (target: 1.30x+)\")\n",
    "    print(f\"Opt Time: {opt_time:.1f}s - {'✓ PASS' if opt_time_pass else '✗ FAIL'} (target: <300s)\")\n",
    "    \n",
    "    if not (speedup_pass and opt_time_pass):\n",
    "        all_passed = False\n",
    "else:\n",
    "    print(\"✗ FAIL - No performance report found\")\n",
    "    all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"OVERALL STATUS: {'✓ ALL TESTS PASSED' if all_passed else '✗ SOME TESTS FAILED'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'platform': 'kaggle',\n",
    "    'all_passed': all_passed,\n",
    "    'accuracy_validation': {\n",
    "        'exists': accuracy_report.exists(),\n",
    "        'passed': acc_pass if accuracy_report.exists() else False\n",
    "    },\n",
    "    'performance_validation': {\n",
    "        'exists': perf_report.exists(),\n",
    "        'passed': (speedup_pass and opt_time_pass) if perf_report.exists() else False\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = Path(\"validation_results/kaggle_validation_summary.json\")\n",
    "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Download Results\n",
    "\n",
    "Download the validation reports to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create archive of results\n",
    "results_dir = Path(\"validation_results\")\n",
    "if results_dir.exists():\n",
    "    archive_name = \"enhanced_ttt_validation_results\"\n",
    "    shutil.make_archive(archive_name, 'zip', results_dir)\n",
    "    print(f\"✓ Created archive: {archive_name}.zip\")\n",
    "    \n",
    "    # In Kaggle, you can download from the output tab\n",
    "    print(\"\\nTo download results:\")\n",
    "    print(\"1. Click the 'Output' tab on the right\")\n",
    "    print(f\"2. Download '{archive_name}.zip'\")\n",
    "    print(\"3. Extract to review detailed reports\")\n",
    "else:\n",
    "    print(\"⚠️ No results directory found\")\n",
    "\n",
    "# List available result files\n",
    "print(\"\\nAvailable result files:\")\n",
    "for file in results_dir.glob(\"*.json\"):\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  - {file.name} ({size_kb:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}