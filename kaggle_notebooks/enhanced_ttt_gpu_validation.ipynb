{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced TTT Strategy - GPU Validation\n",
    "\n",
    "This notebook validates the Enhanced TTT Strategy implementation on Kaggle GPU.\n",
    "\n",
    "**Target Metrics:**\n",
    "- Accuracy: 58%+ on 100 evaluation tasks\n",
    "- Inference Time: <5 minutes per task\n",
    "- Performance Speedup: 30-40% from optimizations\n",
    "\n",
    "**Runtime: ~6-9 hours total**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KAGGLE ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "print(f\"GPU Count: {gpu_count}\")\n",
    "\n",
    "if cuda_available:\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ WARNING: No GPU detected! Enable GPU in notebook settings.\")\n",
    "    print(\"Settings → Accelerator → GPU T4 x2\")\n",
    "\n",
    "# Check Kaggle environment\n",
    "print(f\"\\nKaggle Working Dir: {os.path.exists('/kaggle/working')}\")\n",
    "print(f\"Kaggle Input Dir: {os.path.exists('/kaggle/input')}\")\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\nREPO_URL = \"https://github.com/18h32n/shockabo.git\"\n\nprint(f\"Cloning repository from: {REPO_URL}\")\n\n# Option A: Clone from GitHub (if repo is public)\ntry:\n    !git clone -b master {REPO_URL} /kaggle/working/arc-prize-2025\n    print(\"✓ Repository cloned successfully\")\nexcept Exception as e:\n    print(f\"✗ Clone failed: {e}\")\n    print(\"Checking if directory already exists...\")\n    import os\n    if os.path.exists(\"/kaggle/working/arc-prize-2025\"):\n        print(\"Directory exists, proceeding...\")\n    else:\n        print(\"Alternative: Upload code as Kaggle dataset\")\n        raise\n\n# Option B: If you uploaded code as Kaggle dataset, uncomment:\n# !cp -r /kaggle/input/arc-prize-2025-code/* /kaggle/working/arc-prize-2025/\n\n# Change to project directory\n%cd /kaggle/working/arc-prize-2025\n\n# Verify we're in the right place\n!pwd\n!ls -la\n\n# Run Kaggle setup script\nprint(\"Running Kaggle setup script...\")\ntry:\n    !python scripts/platform_deploy/kaggle_setup.py\n    print(\"✓ Setup script completed\")\nexcept Exception as e:\n    print(f\"⚠️ Setup script issue: {e}\")\n    print(\"Proceeding anyway...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install test dependencies\n",
    "!pip install -q pytest pytest-asyncio pytest-timeout pytest-mock\n",
    "\n",
    "# Install transformers and related packages\n",
    "!pip install -q transformers>=4.35.0 accelerate>=0.25.0 bitsandbytes>=0.41.0 peft>=0.7.0\n",
    "\n",
    "# Verify installations\n",
    "import pytest\n",
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "\n",
    "print(f\"pytest: {pytest.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"accelerate: {accelerate.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(\"\\n✓ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Verify ARC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ARC dataset in Kaggle input\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Common locations for ARC dataset in Kaggle\n",
    "dataset_locations = [\n",
    "    \"/kaggle/input/arc-prize-2025\",\n",
    "    \"/kaggle/input/abstraction-and-reasoning-corpus\",\n",
    "    \"/kaggle/input/arc-agi\",\n",
    "    \"arc-prize-2025/data/downloaded\"  # Local copy\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for loc in dataset_locations:\n",
    "    test_path = Path(loc)\n",
    "    if test_path.exists():\n",
    "        print(f\"Found dataset at: {loc}\")\n",
    "        dataset_path = test_path\n",
    "        break\n",
    "\n",
    "if dataset_path:\n",
    "    # Look for evaluation files\n",
    "    eval_files = list(dataset_path.glob(\"**/arc-agi_evaluation_challenges.json\"))\n",
    "    solution_files = list(dataset_path.glob(\"**/arc-agi_evaluation_solutions.json\"))\n",
    "    \n",
    "    if eval_files and solution_files:\n",
    "        eval_path = eval_files[0]\n",
    "        solution_path = solution_files[0]\n",
    "        \n",
    "        # Load and verify\n",
    "        with open(eval_path) as f:\n",
    "            challenges = json.load(f)\n",
    "        with open(solution_path) as f:\n",
    "            solutions = json.load(f)\n",
    "        \n",
    "        print(f\"\\n✓ Dataset verified!\")\n",
    "        print(f\"  Challenges: {len(challenges)} tasks\")\n",
    "        print(f\"  Solutions: {len(solutions)} tasks\")\n",
    "        print(f\"  Evaluation path: {eval_path}\")\n",
    "        print(f\"  Solutions path: {solution_path}\")\n",
    "        \n",
    "        # Create symlink or copy to expected location\n",
    "        expected_dir = Path(\"arc-prize-2025/data/downloaded\")\n",
    "        expected_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        import shutil\n",
    "        shutil.copy(eval_path, expected_dir / \"arc-agi_evaluation_challenges.json\")\n",
    "        shutil.copy(solution_path, expected_dir / \"arc-agi_evaluation_solutions.json\")\n",
    "        print(f\"\\n✓ Dataset copied to: {expected_dir}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Dataset files not found in {dataset_path}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ ARC dataset not found!\")\n",
    "    print(\"Please add ARC dataset to your notebook:\")\n",
    "    print(\"  1. Click '+ Add Data' button\")\n",
    "    print(\"  2. Search for 'ARC Prize 2025' or 'abstraction reasoning'\")\n",
    "    print(\"  3. Click 'Add' and re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run Enhanced TTT Accuracy Validation\n",
    "\n",
    "**This will take 4-6 hours to complete**\n",
    "\n",
    "Tests 100 evaluation tasks with:\n",
    "- Leave-one-out generation\n",
    "- Self-consistency validation\n",
    "- LoRA optimization\n",
    "- Memory efficient batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run enhanced TTT accuracy test\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nprint(\"Starting Enhanced TTT Accuracy Validation...\")\nprint(\"This will take 4-6 hours to complete\")\nprint(\"=\" * 60)\n\n# Check if test file exists\ntest_file = Path(\"tests/integration/test_enhanced_ttt_accuracy.py\")\nif not test_file.exists():\n    print(f\"✗ Test file not found: {test_file}\")\n    print(\"Available test files:\")\n    for f in Path(\"tests\").rglob(\"*.py\"):\n        print(f\"  - {f}\")\n    sys.exit(1)\n\ntry:\n    # Run the test with proper error handling\n    result = subprocess.run([\n        sys.executable, \"-m\", \"pytest\", \n        \"tests/integration/test_enhanced_ttt_accuracy.py::test_enhanced_ttt_accuracy\",\n        \"-v\", \"-s\", \"--no-header\", \"--tb=short\",\n        \"--timeout=25200\"  # 7 hour timeout\n    ], capture_output=False, text=True, timeout=25200)\n    \n    if result.returncode == 0:\n        print(\"\\n✓ Enhanced TTT Accuracy Test PASSED\")\n    else:\n        print(f\"\\n⚠️ Enhanced TTT Accuracy Test returned code: {result.returncode}\")\n        print(\"Check logs above for details\")\n        \nexcept subprocess.TimeoutExpired:\n    print(\"\\n⚠️ Test timed out after 7 hours\")\nexcept Exception as e:\n    print(f\"\\n✗ Test execution failed: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ENHANCED TTT ACCURACY TEST COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Analyze Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\nprint(\"=\" * 60)\nprint(\"ENHANCED TTT ACCURACY RESULTS\")\nprint(\"=\" * 60)\n\nreport_path = Path(\"validation_results/enhanced_ttt_report.json\")\n\nif not report_path.exists():\n    print(\"⚠️ Results file not found!\")\n    print(f\"Expected: {report_path}\")\n    \n    # Try to find alternative result files\n    results_dir = Path(\"validation_results\")\n    if results_dir.exists():\n        print(\"\\nAvailable result files:\")\n        for f in results_dir.glob(\"*.json\"):\n            print(f\"  - {f}\")\n    else:\n        print(\"No results directory found\")\n        \n    # Check if test ran at all\n    if Path(\"test_output.log\").exists():\n        print(\"\\nTest appears to have run, check logs for issues\")\n    else:\n        print(\"\\nTest may not have completed successfully\")\n        \nelse:\n    try:\n        with open(report_path) as f:\n            report = json.load(f)\n        \n        summary = report.get('summary', {})\n        \n        print(f\"\\nTotal Tasks: {summary.get('total_tasks', 'N/A')}\")\n        print(f\"Correct Tasks: {summary.get('correct_tasks', 'N/A')}\")\n        \n        accuracy = summary.get('accuracy', 0)\n        print(f\"\\nAccuracy: {accuracy:.2%}\")\n        print(f\"Target (58%): {'✓ PASS' if accuracy >= 0.58 else '✗ FAIL'}\")\n        \n        print(f\"\\nAverage Confidence: {summary.get('avg_confidence', 0):.3f}\")\n        print(f\"Average Adaptation Time: {summary.get('avg_adaptation_time_sec', 0):.1f}s\")\n        print(f\"Average Inference Time: {summary.get('avg_inference_time_sec', 0):.1f}s\")\n        \n        total_avg_time = summary.get('avg_adaptation_time_sec', 0) + summary.get('avg_inference_time_sec', 0)\n        print(f\"\\nTotal Average Time: {total_avg_time:.1f}s ({total_avg_time/60:.1f} min)\")\n        print(f\"Time Target (<5 min): {'✓ PASS' if total_avg_time < 300 else '✗ FAIL'}\")\n        \n        print(f\"\\nTotal Runtime: {summary.get('total_time_sec', 0)/3600:.2f} hours\")\n        \n        # Difficulty breakdown\n        if 'difficulty_breakdown' in report:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"DIFFICULTY BREAKDOWN\")\n            print(\"=\" * 60)\n            for difficulty, metrics in report['difficulty_breakdown'].items():\n                print(f\"\\n{difficulty.upper()}:\")\n                print(f\"  Tasks: {metrics.get('total', 'N/A')}\")\n                print(f\"  Correct: {metrics.get('correct', 'N/A')}\")\n                print(f\"  Accuracy: {metrics.get('accuracy', 0):.2%}\")\n                print(f\"  Avg Confidence: {metrics.get('avg_confidence', 0):.3f}\")\n                print(f\"  Avg Time: {metrics.get('avg_time', 0):.1f}s\")\n        \n        print(\"\\n\" + \"=\" * 60)\n        \n    except Exception as e:\n        print(f\"✗ Error reading results file: {e}\")\n        print(\"File may be corrupted or incomplete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run Performance Optimization Benchmarks\n",
    "\n",
    "**This will take 2-3 hours to complete**\n",
    "\n",
    "Benchmarks optimization techniques:\n",
    "- KV-cache optimization\n",
    "- Static cache\n",
    "- torch.compile\n",
    "- Combined speedup measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance optimization test\n",
    "!pytest tests/performance/test_ttt_inference_time.py::test_combined_optimizations \\\n",
    "    -v -s --no-header --tb=short \\\n",
    "    --timeout=14400  # 4 hour timeout\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE OPTIMIZATION TEST COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Analyze Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "perf_report_path = Path(\"validation_results/ttt_inference_optimization_report.json\")\n",
    "\n",
    "if perf_report_path.exists():\n",
    "    with open(perf_report_path) as f:\n",
    "        perf_report = json.load(f)\n",
    "    \n",
    "    speedup = perf_report['speedup_summary']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PERFORMANCE OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nBaseline Average Time: {speedup['avg_baseline_sec']:.1f}s\")\n",
    "    print(f\"Optimized Average Time: {speedup['avg_optimized_sec']:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nSpeedup Factor: {speedup['speedup_factor']:.2f}x\")\n",
    "    print(f\"Time Reduction: {speedup['reduction_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nTarget Speedup (1.30x): {'✓ PASS' if speedup['speedup_factor'] >= 1.30 else '✗ FAIL'}\")\n",
    "    print(f\"Target Time (<5 min): {'✓ PASS' if speedup['avg_optimized_sec'] < 300 else '✗ FAIL'}\")\n",
    "    \n",
    "    print(f\"\\nBaseline Samples: {speedup['baseline_samples']}\")\n",
    "    print(f\"Optimized Samples: {speedup['optimized_samples']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Performance results file not found!\")\n",
    "    print(f\"Expected: {perf_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Final Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Platform: Kaggle\")\n",
    "\n",
    "# Check accuracy results\n",
    "accuracy_report = Path(\"validation_results/enhanced_ttt_report.json\")\n",
    "perf_report = Path(\"validation_results/ttt_inference_optimization_report.json\")\n",
    "\n",
    "all_passed = True\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ACCURACY VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if accuracy_report.exists():\n",
    "    with open(accuracy_report) as f:\n",
    "        acc_data = json.load(f)\n",
    "    \n",
    "    accuracy = acc_data['summary']['accuracy']\n",
    "    avg_time = (acc_data['summary']['avg_adaptation_time_sec'] + \n",
    "                acc_data['summary']['avg_inference_time_sec'])\n",
    "    \n",
    "    acc_pass = accuracy >= 0.58\n",
    "    time_pass = avg_time < 300\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2%} - {'✓ PASS' if acc_pass else '✗ FAIL'} (target: 58%+)\")\n",
    "    print(f\"Avg Time: {avg_time:.1f}s - {'✓ PASS' if time_pass else '✗ FAIL'} (target: <300s)\")\n",
    "    \n",
    "    if not (acc_pass and time_pass):\n",
    "        all_passed = False\n",
    "else:\n",
    "    print(\"✗ FAIL - No accuracy report found\")\n",
    "    all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"PERFORMANCE OPTIMIZATION VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if perf_report.exists():\n",
    "    with open(perf_report) as f:\n",
    "        perf_data = json.load(f)\n",
    "    \n",
    "    speedup_factor = perf_data['speedup_summary']['speedup_factor']\n",
    "    opt_time = perf_data['speedup_summary']['avg_optimized_sec']\n",
    "    \n",
    "    speedup_pass = speedup_factor >= 1.30\n",
    "    opt_time_pass = opt_time < 300\n",
    "    \n",
    "    print(f\"Speedup: {speedup_factor:.2f}x - {'✓ PASS' if speedup_pass else '✗ FAIL'} (target: 1.30x+)\")\n",
    "    print(f\"Opt Time: {opt_time:.1f}s - {'✓ PASS' if opt_time_pass else '✗ FAIL'} (target: <300s)\")\n",
    "    \n",
    "    if not (speedup_pass and opt_time_pass):\n",
    "        all_passed = False\n",
    "else:\n",
    "    print(\"✗ FAIL - No performance report found\")\n",
    "    all_passed = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"OVERALL STATUS: {'✓ ALL TESTS PASSED' if all_passed else '✗ SOME TESTS FAILED'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'platform': 'kaggle',\n",
    "    'all_passed': all_passed,\n",
    "    'accuracy_validation': {\n",
    "        'exists': accuracy_report.exists(),\n",
    "        'passed': acc_pass if accuracy_report.exists() else False\n",
    "    },\n",
    "    'performance_validation': {\n",
    "        'exists': perf_report.exists(),\n",
    "        'passed': (speedup_pass and opt_time_pass) if perf_report.exists() else False\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = Path(\"validation_results/kaggle_validation_summary.json\")\n",
    "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Download Results\n",
    "\n",
    "Download the validation reports to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable archive\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create archive of results\n",
    "results_dir = Path(\"validation_results\")\n",
    "if results_dir.exists():\n",
    "    archive_name = \"enhanced_ttt_validation_results\"\n",
    "    shutil.make_archive(archive_name, 'zip', results_dir)\n",
    "    print(f\"✓ Created archive: {archive_name}.zip\")\n",
    "    \n",
    "    # In Kaggle, you can download from the output tab\n",
    "    print(\"\\nTo download results:\")\n",
    "    print(\"1. Click the 'Output' tab on the right\")\n",
    "    print(f\"2. Download '{archive_name}.zip'\")\n",
    "    print(\"3. Extract to review detailed reports\")\n",
    "else:\n",
    "    print(\"⚠️ No results directory found\")\n",
    "\n",
    "# List available result files\n",
    "print(\"\\nAvailable result files:\")\n",
    "for file in results_dir.glob(\"*.json\"):\n",
    "    size_kb = file.stat().st_size / 1024\n",
    "    print(f\"  - {file.name} ({size_kb:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}