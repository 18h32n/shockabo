"""Demonstration and testing script for the centralized error handling system.

This script shows how to use the error handling framework and tests various scenarios
to ensure robust error management across the ARC Prize evaluation system.
"""

import asyncio
import json
import logging
import random
import time
from datetime import datetime
from typing import Dict, List, Optional

import structlog

from .error_handling import (
    ARCBaseException,
    DataNotFoundException,
    DataCorruptionException,
    EvaluationException,
    TaskNotFoundException,
    AuthenticationException,
    ErrorCode,
    ErrorSeverity,
    ErrorContext,
    ErrorResponse,
    ErrorLogger,
    create_error_response,
)
from .error_recovery import (
    CircuitBreaker,
    CircuitBreakerConfig,
    RetryStrategy,
    FallbackStrategy,
    HealthMonitor,
    get_circuit_breaker,
    get_health_monitor,
)


# Configure logging for demo
logging.basicConfig(level=logging.INFO)
logger = structlog.get_logger(__name__)


class ErrorHandlingDemo:
    """Demonstration class for error handling features."""
    
    def __init__(self):
        self.error_logger = ErrorLogger()
        self.demo_results: Dict[str, Dict] = {}
    
    async def run_all_demos(self) -> Dict[str, Dict]:
        """Run all error handling demonstrations."""
        logger.info("starting_error_handling_demos")
        
        demos = [
            ("basic_exception_handling", self.demo_basic_exceptions),
            ("custom_arc_exceptions", self.demo_custom_exceptions),
            ("error_logging", self.demo_error_logging),
            ("circuit_breaker", self.demo_circuit_breaker),
            ("retry_mechanisms", self.demo_retry_mechanisms),
            ("fallback_strategies", self.demo_fallback_strategies),
            ("health_monitoring", self.demo_health_monitoring),
            ("api_error_responses", self.demo_api_error_responses),
            ("recovery_patterns", self.demo_recovery_patterns),
        ]
        
        for demo_name, demo_func in demos:
            try:
                logger.info(f"running_demo", demo=demo_name)
                start_time = time.time()
                
                result = await demo_func()
                
                execution_time = time.time() - start_time
                self.demo_results[demo_name] = {
                    "status": "success",
                    "result": result,
                    "execution_time": execution_time
                }
                
                logger.info(
                    f"demo_completed",
                    demo=demo_name,
                    execution_time_ms=execution_time * 1000
                )
                
            except Exception as e:
                execution_time = time.time() - start_time
                self.demo_results[demo_name] = {
                    "status": "error",
                    "error": str(e),
                    "execution_time": execution_time
                }
                
                logger.error(
                    f"demo_failed",
                    demo=demo_name,
                    error=str(e),
                    exc_info=True
                )
        
        logger.info("all_demos_completed", total_demos=len(demos))\n        return self.demo_results
    
    async def demo_basic_exceptions(self) -> Dict:
        """Demonstrate basic exception handling patterns."""
        results = {"tests": []}\n        \n        # Test 1: Basic ARCBaseException\n        try:\n            raise ARCBaseException(\n                message=\"This is a demo exception\",\n                error_code=ErrorCode.VALIDATION_ERROR,\n                severity=ErrorSeverity.MEDIUM,\n                suggestions=[\"This is just a demo\", \"No action needed\"]\n            )\n        except ARCBaseException as e:\n            results[\"tests\"].append({\n                \"name\": \"basic_arc_exception\",\n                \"status\": \"caught\",\n                \"error_code\": e.error_code.value,\n                \"severity\": e.severity.value,\n                \"suggestions_count\": len(e.suggestions)\n            })\n        \n        # Test 2: Exception with context\n        try:\n            context = ErrorContext(\n                user_id=\"demo_user\",\n                task_id=\"demo_task_123\",\n                additional_data={\"demo\": True, \"timestamp\": datetime.now().isoformat()}\n            )\n            \n            raise DataNotFoundException(\n                \"task\",\n                \"demo_task_123\",\n                context=context\n            )\n        except DataNotFoundException as e:\n            results[\"tests\"].append({\n                \"name\": \"exception_with_context\",\n                \"status\": \"caught\",\n                \"has_context\": e.context is not None,\n                \"context_user_id\": e.context.user_id if e.context else None,\n                \"context_task_id\": e.context.task_id if e.context else None\n            })\n        \n        # Test 3: Exception chaining\n        try:\n            try:\n                # Simulate original error\n                raise ValueError(\"Original error occurred\")\n            except ValueError as original_error:\n                raise EvaluationException(\n                    \"demo_task\",\n                    \"Evaluation failed due to underlying issue\",\n                    cause=original_error\n                ) from original_error\n        except EvaluationException as e:\n            results[\"tests\"].append({\n                \"name\": \"exception_chaining\",\n                \"status\": \"caught\",\n                \"has_cause\": e.cause is not None,\n                \"cause_type\": type(e.cause).__name__ if e.cause else None\n            })\n        \n        return results
    
    async def demo_custom_exceptions(self) -> Dict:\n        \"\"\"Demonstrate custom ARC exception types.\"\"\"\n        results = {\"exception_types\": []}\n        \n        # Define test scenarios for different exception types\n        exception_scenarios = [\n            {\n                \"name\": \"TaskNotFoundException\",\n                \"exception\": TaskNotFoundException(\"invalid_task_id\"),\n                \"expected_code\": ErrorCode.TASK_NOT_FOUND\n            },\n            {\n                \"name\": \"DataCorruptionException\",\n                \"exception\": DataCorruptionException(\n                    \"task_file\", \n                    \"Invalid JSON format detected\"\n                ),\n                \"expected_code\": ErrorCode.DATA_CORRUPTION\n            },\n            {\n                \"name\": \"AuthenticationException\",\n                \"exception\": AuthenticationException(\"Invalid token provided\"),\n                \"expected_code\": ErrorCode.INVALID_TOKEN\n            },\n            {\n                \"name\": \"EvaluationException\",\n                \"exception\": EvaluationException(\n                    \"test_task\", \n                    \"Pixel accuracy calculation failed\"\n                ),\n                \"expected_code\": ErrorCode.EVALUATION_ERROR\n            }\n        ]\n        \n        for scenario in exception_scenarios:\n            try:\n                raise scenario[\"exception\"]\n            except ARCBaseException as e:\n                results[\"exception_types\"].append({\n                    \"name\": scenario[\"name\"],\n                    \"error_code_match\": e.error_code == scenario[\"expected_code\"],\n                    \"has_suggestions\": len(e.suggestions) > 0,\n                    \"error_id_generated\": bool(e.error_id),\n                    \"to_dict_works\": bool(e.to_dict()),\n                    \"to_response_works\": isinstance(e.to_response(), ErrorResponse)\n                })\n        \n        return results\n    \n    async def demo_error_logging(self) -> Dict:\n        \"\"\"Demonstrate structured error logging.\"\"\"\n        results = {\"logging_tests\": []}\n        \n        # Test 1: Basic error logging\n        test_exception = ARCBaseException(\n            message=\"Demo error for logging test\",\n            error_code=ErrorCode.VALIDATION_ERROR,\n            context=ErrorContext(\n                user_id=\"demo_user\",\n                additional_data={\"test\": \"logging_demo\"}\n            )\n        )\n        \n        error_id = self.error_logger.log_error(test_exception)\n        \n        results[\"logging_tests\"].append({\n            \"name\": \"basic_error_logging\",\n            \"error_id_returned\": bool(error_id),\n            \"error_id_format\": \"uuid\" if len(error_id.split(\"-\")) == 5 else \"other\"\n        })\n        \n        # Test 2: Logging with additional context\n        additional_context = {\n            \"request_ip\": \"127.0.0.1\",\n            \"user_agent\": \"Demo/1.0\",\n            \"request_path\": \"/api/demo\"\n        }\n        \n        error_id_2 = self.error_logger.log_error(\n            test_exception,\n            additional_context=additional_context\n        )\n        \n        results[\"logging_tests\"].append({\n            \"name\": \"logging_with_context\",\n            \"error_id_returned\": bool(error_id_2),\n            \"different_error_ids\": error_id != error_id_2\n        })\n        \n        return results\n    \n    async def demo_circuit_breaker(self) -> Dict:\n        \"\"\"Demonstrate circuit breaker functionality.\"\"\"\n        results = {\"circuit_breaker_tests\": []}\n        \n        # Create a circuit breaker with low thresholds for demo\n        config = CircuitBreakerConfig(\n            failure_threshold=3,\n            recovery_timeout=2,  # 2 seconds for demo\n            success_threshold=2\n        )\n        \n        circuit_breaker = get_circuit_breaker(\"demo_service\", config)\n        \n        # Test 1: Successful operations\n        async def successful_operation():\n            await asyncio.sleep(0.1)\n            return \"success\"\n        \n        try:\n            result = await circuit_breaker.call(successful_operation)\n            results[\"circuit_breaker_tests\"].append({\n                \"name\": \"successful_operation\",\n                \"status\": \"success\",\n                \"result\": result\n            })\n        except Exception as e:\n            results[\"circuit_breaker_tests\"].append({\n                \"name\": \"successful_operation\",\n                \"status\": \"error\",\n                \"error\": str(e)\n            })\n        \n        # Test 2: Failing operations to trip circuit breaker\n        async def failing_operation():\n            raise Exception(\"Simulated service failure\")\n        \n        failure_count = 0\n        for i in range(5):  # Try to trip the circuit breaker\n            try:\n                await circuit_breaker.call(failing_operation)\n            except Exception:\n                failure_count += 1\n        \n        results[\"circuit_breaker_tests\"].append({\n            \"name\": \"trip_circuit_breaker\",\n            \"failures_recorded\": failure_count,\n            \"circuit_tripped\": failure_count >= config.failure_threshold\n        })\n        \n        # Test 3: Circuit breaker should reject requests when open\n        try:\n            await circuit_breaker.call(successful_operation)\n            circuit_rejected = False\n        except ARCBaseException as e:\n            circuit_rejected = \"circuit breaker\" in str(e).lower()\n        \n        results[\"circuit_breaker_tests\"].append({\n            \"name\": \"circuit_rejection\",\n            \"rejected_when_open\": circuit_rejected\n        })\n        \n        # Get circuit breaker stats\n        stats = circuit_breaker.get_stats()\n        results[\"circuit_breaker_stats\"] = {\n            \"state\": stats[\"state\"],\n            \"total_requests\": stats[\"total_requests\"],\n            \"total_failures\": stats[\"total_failures\"]\n        }\n        \n        return results\n    \n    async def demo_retry_mechanisms(self) -> Dict:\n        \"\"\"Demonstrate retry strategies.\"\"\"\n        results = {\"retry_tests\": []}\n        \n        # Test 1: Retry with eventual success\n        attempt_count = 0\n        \n        async def eventually_succeeds():\n            nonlocal attempt_count\n            attempt_count += 1\n            if attempt_count < 3:\n                raise Exception(f\"Attempt {attempt_count} failed\")\n            return f\"Success on attempt {attempt_count}\"\n        \n        retry_strategy = RetryStrategy(\n            max_attempts=5,\n            base_delay=0.1,  # Fast for demo\n            backoff_multiplier=1.5\n        )\n        \n        try:\n            result = await retry_strategy.execute(eventually_succeeds)\n            results[\"retry_tests\"].append({\n                \"name\": \"eventual_success\",\n                \"status\": \"success\",\n                \"attempts_needed\": attempt_count,\n                \"result\": result\n            })\n        except Exception as e:\n            results[\"retry_tests\"].append({\n                \"name\": \"eventual_success\",\n                \"status\": \"failed\",\n                \"attempts_made\": attempt_count,\n                \"error\": str(e)\n            })\n        \n        # Test 2: Retry with non-retryable exception\n        async def non_retryable_failure():\n            raise ValueError(\"This should not be retried\")\n        \n        non_retry_strategy = RetryStrategy(\n            max_attempts=3,\n            non_retryable_exceptions=(ValueError,)\n        )\n        \n        try:\n            await non_retry_strategy.execute(non_retryable_failure)\n            results[\"retry_tests\"].append({\n                \"name\": \"non_retryable\",\n                \"status\": \"unexpected_success\"\n            })\n        except Exception:\n            results[\"retry_tests\"].append({\n                \"name\": \"non_retryable\",\n                \"status\": \"correctly_not_retried\"\n            })\n        \n        return results\n    \n    async def demo_fallback_strategies(self) -> Dict:\n        \"\"\"Demonstrate fallback mechanisms.\"\"\"\n        results = {\"fallback_tests\": []}\n        \n        fallback_strategy = FallbackStrategy(\"demo_data_access\")\n        \n        # Add fallback functions\n        async def primary_data_source():\n            raise Exception(\"Primary database is down\")\n        \n        async def cache_fallback():\n            await asyncio.sleep(0.1)\n            return \"data_from_cache\"\n        \n        async def static_fallback():\n            return \"default_static_data\"\n        \n        # Add fallbacks with priorities\n        fallback_strategy.add_fallback(cache_fallback, priority=2)\n        fallback_strategy.add_fallback(static_fallback, priority=1)\n        \n        # Test fallback execution\n        try:\n            result = await fallback_strategy.execute(primary_data_source)\n            results[\"fallback_tests\"].append({\n                \"name\": \"primary_fails_fallback_succeeds\",\n                \"status\": \"success\",\n                \"result\": result,\n                \"used_fallback\": result != \"primary_data\"\n            })\n        except Exception as e:\n            results[\"fallback_tests\"].append({\n                \"name\": \"primary_fails_fallback_succeeds\",\n                \"status\": \"failed\",\n                \"error\": str(e)\n            })\n        \n        # Get fallback stats\n        stats = fallback_strategy.get_stats()\n        results[\"fallback_stats\"] = {\n            \"name\": stats[\"name\"],\n            \"fallback_count\": stats[\"fallback_count\"],\n            \"primary_success_rate\": stats[\"primary_success_rate\"],\n            \"fallback_usage_rate\": stats[\"fallback_usage_rate\"]\n        }\n        \n        return results\n    \n    async def demo_health_monitoring(self) -> Dict:\n        \"\"\"Demonstrate health monitoring.\"\"\"\n        results = {\"health_tests\": []}\n        \n        health_monitor = get_health_monitor(\"demo_system\", check_interval=1)\n        \n        # Add health checks\n        database_healthy = True\n        cache_healthy = True\n        \n        def check_database():\n            return database_healthy\n        \n        def check_cache():\n            return cache_healthy\n        \n        def recover_database():\n            nonlocal database_healthy\n            database_healthy = True\n            logger.info(\"database_recovered\")\n        \n        health_monitor.add_health_check(\n            \"database\", \n            check_database, \n            recovery_func=recover_database,\n            critical=True\n        )\n        health_monitor.add_health_check(\n            \"cache\", \n            check_cache,\n            critical=False\n        )\n        \n        # Start monitoring\n        await health_monitor.start_monitoring()\n        \n        # Wait for initial health check\n        await asyncio.sleep(1.5)\n        \n        # Check initial status\n        initial_status = health_monitor.get_health_status()\n        results[\"health_tests\"].append({\n            \"name\": \"initial_healthy_status\",\n            \"overall_healthy\": initial_status[\"overall_healthy\"],\n            \"monitoring\": initial_status[\"monitoring\"]\n        })\n        \n        # Simulate database failure\n        database_healthy = False\n        await asyncio.sleep(1.5)\n        \n        # Check unhealthy status\n        unhealthy_status = health_monitor.get_health_status()\n        results[\"health_tests\"].append({\n            \"name\": \"unhealthy_detection\",\n            \"overall_healthy\": unhealthy_status[\"overall_healthy\"],\n            \"database_healthy\": unhealthy_status[\"components\"][\"database\"][\"healthy\"]\n        })\n        \n        # Wait for recovery attempt\n        await asyncio.sleep(2)\n        \n        # Check recovery\n        recovered_status = health_monitor.get_health_status()\n        results[\"health_tests\"].append({\n            \"name\": \"auto_recovery\",\n            \"overall_healthy\": recovered_status[\"overall_healthy\"],\n            \"database_healthy\": recovered_status[\"components\"][\"database\"][\"healthy\"]\n        })\n        \n        # Stop monitoring\n        await health_monitor.stop_monitoring()\n        \n        return results\n    \n    async def demo_api_error_responses(self) -> Dict:\n        \"\"\"Demonstrate API error response formatting.\"\"\"\n        results = {\"api_response_tests\": []}\n        \n        # Test 1: Convert exception to API response\n        test_exception = TaskNotFoundException(\n            \"missing_task_123\",\n            context=ErrorContext(user_id=\"api_user\")\n        )\n        \n        # This would normally be handled by middleware\n        response = create_error_response(test_exception, 404)\n        \n        results[\"api_response_tests\"].append({\n            \"name\": \"exception_to_api_response\",\n            \"status_code\": response.status_code,\n            \"has_error_id_header\": \"X-Error-ID\" in response.headers,\n            \"content_type\": \"application/json\"\n        })\n        \n        # Test 2: Verify response structure\n        try:\n            # In a real scenario, this would be JSON\n            # For demo, we'll check the structure\n            response_structure_valid = all([\n                hasattr(test_exception.to_response(), \"error_id\"),\n                hasattr(test_exception.to_response(), \"error_code\"),\n                hasattr(test_exception.to_response(), \"message\"),\n                hasattr(test_exception.to_response(), \"suggestions\")\n            ])\n            \n            results[\"api_response_tests\"].append({\n                \"name\": \"response_structure\",\n                \"structure_valid\": response_structure_valid,\n                \"error_response_serializable\": True\n            })\n        except Exception as e:\n            results[\"api_response_tests\"].append({\n                \"name\": \"response_structure\",\n                \"structure_valid\": False,\n                \"error\": str(e)\n            })\n        \n        return results\n    \n    async def demo_recovery_patterns(self) -> Dict:\n        \"\"\"Demonstrate advanced recovery patterns.\"\"\"\n        results = {\"recovery_tests\": []}\n        \n        # Test 1: Combined circuit breaker + retry + fallback\n        circuit_breaker = get_circuit_breaker(\"recovery_demo\")\n        retry_strategy = RetryStrategy(max_attempts=2, base_delay=0.1)\n        \n        async def unreliable_service():\n            if random.random() < 0.7:  # 70% failure rate\n                raise Exception(\"Service temporarily unavailable\")\n            return \"service_response\"\n        \n        async def fallback_response():\n            return \"fallback_response\"\n        \n        # Combine patterns\n        attempts = 0\n        successes = 0\n        \n        for _ in range(10):  # Try multiple times\n            try:\n                attempts += 1\n                # Try primary service with circuit breaker and retry\n                result = await retry_strategy.execute(\n                    lambda: circuit_breaker.call(unreliable_service)\n                )\n                successes += 1\n            except Exception:\n                # Use fallback\n                result = await fallback_response()\n                successes += 1\n        \n        results[\"recovery_tests\"].append({\n            \"name\": \"combined_recovery_patterns\",\n            \"attempts\": attempts,\n            \"successes\": successes,\n            \"success_rate\": successes / attempts if attempts > 0 else 0\n        })\n        \n        return results\n    \n    def generate_demo_report(self) -> str:\n        \"\"\"Generate a comprehensive demo report.\"\"\"\n        report = []\n        report.append(\"# ARC Prize Error Handling System Demo Report\")\n        report.append(f\"Generated: {datetime.now().isoformat()}\")\n        report.append(\"\\n## Demo Results Summary\\n\")\n        \n        total_demos = len(self.demo_results)\n        successful_demos = sum(1 for r in self.demo_results.values() if r[\"status\"] == \"success\")\n        \n        report.append(f\"- Total demos run: {total_demos}\")\n        report.append(f\"- Successful demos: {successful_demos}\")\n        report.append(f\"- Success rate: {successful_demos/total_demos*100:.1f}%\")\n        \n        # Detailed results\n        for demo_name, result in self.demo_results.items():\n            report.append(f\"\\n### {demo_name.replace('_', ' ').title()}\\n\")\n            report.append(f\"- Status: {result['status']}\")\n            report.append(f\"- Execution time: {result['execution_time']*1000:.2f}ms\")\n            \n            if result[\"status\"] == \"success\" and \"result\" in result:\n                # Add key metrics from each demo\n                demo_result = result[\"result\"]\n                \n                if \"tests\" in demo_result:\n                    report.append(f\"- Tests run: {len(demo_result['tests'])}\")\n                \n                if \"exception_types\" in demo_result:\n                    report.append(f\"- Exception types tested: {len(demo_result['exception_types'])}\")\n                \n                if \"circuit_breaker_stats\" in demo_result:\n                    stats = demo_result[\"circuit_breaker_stats\"]\n                    report.append(f\"- Circuit breaker state: {stats['state']}\")\n                    report.append(f\"- Total requests: {stats['total_requests']}\")\n            \n            elif result[\"status\"] == \"error\":\n                report.append(f\"- Error: {result['error']}\")\n        \n        report.append(\"\\n## Key Features Demonstrated\\n\")\n        report.append(\"- ✅ Custom exception hierarchy with error codes\")\n        report.append(\"- ✅ Structured error logging with context\")\n        report.append(\"- ✅ Circuit breaker patterns for service resilience\")\n        report.append(\"- ✅ Retry mechanisms with exponential backoff\")\n        report.append(\"- ✅ Fallback strategies for graceful degradation\")\n        report.append(\"- ✅ Health monitoring and auto-recovery\")\n        report.append(\"- ✅ API error response standardization\")\n        report.append(\"- ✅ Combined recovery patterns\")\n        \n        report.append(\"\\n## Usage Recommendations\\n\")\n        report.append(\"1. Use ARCBaseException and its subclasses for all domain-specific errors\")\n        report.append(\"2. Include ErrorContext with user_id, task_id, and relevant metadata\")\n        report.append(\"3. Implement circuit breakers for external service calls\")\n        report.append(\"4. Use retry strategies for transient failures\")\n        report.append(\"5. Set up health monitoring for critical system components\")\n        report.append(\"6. Let middleware handle error response formatting\")\n        \n        return \"\\n\".join(report)\n\n\nasync def main():\n    \"\"\"Run the error handling demonstration.\"\"\"\n    print(\"Starting ARC Prize Error Handling System Demo...\")\n    \n    demo = ErrorHandlingDemo()\n    results = await demo.run_all_demos()\n    \n    # Print summary\n    print(f\"\\nDemo completed! Results:\")\n    for demo_name, result in results.items():\n        status_emoji = \"✅\" if result[\"status\"] == \"success\" else \"❌\"\n        print(f\"{status_emoji} {demo_name}: {result['status']} ({result['execution_time']*1000:.1f}ms)\")\n    \n    # Generate and print report\n    report = demo.generate_demo_report()\n    print(\"\\n\" + \"=\"*80)\n    print(report)\n    print(\"=\"*80)\n    \n    return results\n\n\nif __name__ == \"__main__\":\n    # Run the demo\n    results = asyncio.run(main())